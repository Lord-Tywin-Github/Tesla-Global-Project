{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32360cd0-c729-4f87-8737-80e932a796ac",
   "metadata": {},
   "source": [
    "## **1. 基本训练代码模板** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d8a2757-d7ce-478a-b5c9-ed5fd1cef30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始训练 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 00:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.672177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.653839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.645104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 训练完成 ---\n"
     ]
    }
   ],
   "source": [
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetuned_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",  # 已将 'evaluation_strategy' 更改为 'eval_strategy'\n",
    "    save_strategy=\"epoch\",  # 已将 'save_strategy' 更改为 'save_strategy'\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# 创建Trainer实例\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    ")\n",
    "\n",
    "# 开始训练\n",
    "print(\"--- 开始训练 ---\")\n",
    "trainer.train()\n",
    "print(\"--- 训练完成 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f47ca32-309c-47b6-877f-4ef0321c2c1e",
   "metadata": {},
   "source": [
    "## **2. 模型打包** ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e9f9d0e-4b80-4f2b-9866-f2dd94a6e9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from /home/tywin/my_jupyter_project/gemma-2b...\n",
      "Loading model from /home/tywin/my_jupyter_project/gemma-2b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# The path to the folder where you saved the model files\n",
    "local_model_path = \"/home/tywin/my_jupyter_project/gemma-2b\"\n",
    "\n",
    "# Load the tokenizer from the local folder\n",
    "print(f\"Loading tokenizer from {local_model_path}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "# Load the model from the local folder\n",
    "print(f\"Loading model from {local_model_path}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f660f8-3bf3-4ce6-9f4f-0269c4d3db32",
   "metadata": {},
   "source": [
    "# **3. 训练模型** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2329730-298f-4d32-aeab-7965409c83af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./output_data/Fact_Sales.csv...\n",
      "Sampling the first 1000 rows for training.\n",
      "Warning: 'ProductKey' or 'SalesAmount' column not found. Using generic formatting.\n",
      "Data loaded and formatted successfully.\n",
      "Total training examples: 1000\n",
      "Using device: cuda\n",
      "Loading tokenizer from /home/tywin/my_jupyter_project/gemma-2b...\n",
      "Loading model from /home/tywin/my_jupyter_project/gemma-2b with 4-bit quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 2/2 [00:03<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LoRA 模型配置完成 ---\n",
      "trainable params: 19,611,648 || all params: 2,525,784,064 || trainable%: 0.7765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 22227.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始训练 Gemma 模型 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/tywin/my_jupyter_project/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 1:06:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.845300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.316800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.196900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tywin/my_jupyter_project/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/tywin/my_jupyter_project/venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 训练完成 ---\n",
      "LoRA 适配器已保存到 ./my_gemma_lora_adapter 目录。\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "\n",
    "# ---\n",
    "# 1. 加载和准备数据\n",
    "# ---\n",
    "\n",
    "csv_file_path = \"./output_data/Fact_Sales.csv\"\n",
    "print(f\"Loading data from {csv_file_path}...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(csv_file_path, on_bad_lines='skip')\n",
    "    \n",
    "    if df.empty:\n",
    "        raise ValueError(\"The CSV file is empty. Please check the file content.\")\n",
    "    \n",
    "    # --- 只取前1000行 ---\n",
    "    df = df.head(1000)\n",
    "    print(f\"Sampling the first {len(df)} rows for training.\")\n",
    "\n",
    "    # --- 格式化训练数据 ---\n",
    "    if 'ProductKey' in df.columns and 'SalesAmount' in df.columns:\n",
    "        df['text'] = \"### 用户: 产品ID \" + df['ProductKey'].astype(str) + \" 的销售额是多少？\\n### 助手: \" + df['SalesAmount'].astype(str)\n",
    "        print(\"Data formatted using 'ProductKey' and 'SalesAmount' columns.\")\n",
    "    else:\n",
    "        print(\"Warning: 'ProductKey' or 'SalesAmount' column not found. Using generic formatting.\")\n",
    "        df['text'] = \"### 用户: 告诉我关于这行数据的信息。\\n### 助手: \" + df.astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(df)\n",
    "    print(\"Data loaded and formatted successfully.\")\n",
    "    print(f\"Total training examples: {len(train_dataset)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {csv_file_path} was not found. Please check the path.\")\n",
    "    exit()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the data: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ---\n",
    "# 2. 加载模型和分词器\n",
    "# ---\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "local_model_path = \"/home/tywin/my_jupyter_project/gemma-2b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(f\"Loading tokenizer from {local_model_path}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "print(f\"Loading model from {local_model_path} with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "# ---\n",
    "# 3. 准备 LoRA 配置和分词数据\n",
    "# ---\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"--- LoRA 模型配置完成 ---\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_output = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokenized_output['labels'] = tokenized_output['input_ids'].copy()\n",
    "    return tokenized_output\n",
    "\n",
    "tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "# ---\n",
    "# 4. 训练模型\n",
    "# ---\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma_finetuned\",\n",
    "    learning_rate=2e-4,\n",
    "    # 将批量大小减小到2\n",
    "    per_device_train_batch_size=2, \n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"--- 开始训练 Gemma 模型 ---\")\n",
    "trainer.train()\n",
    "print(\"--- 训练完成 ---\")\n",
    "\n",
    "trainer.save_model(\"./my_gemma_lora_adapter\")\n",
    "print(\"LoRA 适配器已保存到 ./my_gemma_lora_adapter 目录。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded23eb0-ac84-40ce-9d9b-b706235e0eaf",
   "metadata": {},
   "source": [
    "# **4. 清空显存** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c90f4ea9-ba48-4058-b396-28d221f47124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU显存已清空。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. 删除不再需要的变量\n",
    "# 例如：\n",
    "# del old_model\n",
    "# del old_optimizer\n",
    "# del old_dataset\n",
    "\n",
    "# 2. 调用 PyTorch 的显存缓存清空函数\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU显存已清空。\")\n",
    "else:\n",
    "    print(\"没有可用的GPU。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9672f90-91e2-425b-92b0-a47852a41e11",
   "metadata": {},
   "source": [
    "# **5. 训练所有表的前200行数据** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41793d53-5fec-4fea-a1fd-d6e48d0a1c7c",
   "metadata": {},
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# ---\n",
    "# 1. 从所有 CSV 文件中加载数据并分割\n",
    "# ---\n",
    "\n",
    "data_dir = \"./output_data\"\n",
    "all_df = []\n",
    "\n",
    "print(f\"Loading first 200 rows from all CSV files in {data_dir}...\")\n",
    "\n",
    "for file in glob.glob(os.path.join(data_dir, '*.csv')):\n",
    "    try:\n",
    "        df = pd.read_csv(file, nrows=500)\n",
    "        all_df.append(df)\n",
    "        print(f\"  Loaded {file} with {len(df)} rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Could not load {file}: {e}\")\n",
    "\n",
    "if all_df:\n",
    "    merged_df = pd.concat(all_df, ignore_index=True)\n",
    "    print(f\"\\nAll files merged into a single DataFrame with {len(merged_df)} rows.\")\n",
    "else:\n",
    "    print(\"No CSV files found. Please check your data directory. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# 格式化训练数据\n",
    "merged_df['text'] = \"### 用户: 告诉我关于这行数据的信息。\\n### 助手: \" + merged_df.astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# 将 DataFrame 转换为 Hugging Face Dataset\n",
    "full_dataset = Dataset.from_pandas(merged_df)\n",
    "\n",
    "# 将数据集分割为训练集和验证集（90/10）\n",
    "split_datasets = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']\n",
    "print(f\"Dataset split: {len(train_dataset)} training examples, {len(eval_dataset)} validation examples.\")\n",
    "\n",
    "# ---\n",
    "# 2. 加载模型和分词器\n",
    "# ---\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "local_model_path = \"/home/tywin/my_jupyter_project/gemma-2b\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(f\"Loading tokenizer from {local_model_path}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "\n",
    "print(f\"Loading model from {local_model_path} with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "# ---\n",
    "# 3. 准备 LoRA 配置和分词数据\n",
    "# ---\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"--- LoRA 模型配置完成 ---\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_output = tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokenized_output['labels'] = tokenized_output['input_ids'].copy()\n",
    "    return tokenized_output\n",
    "\n",
    "# 对训练集和验证集进行分词\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "# ---\n",
    "# 4. 训练模型\n",
    "# ---\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma_finetuned\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    # 启用评估策略\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=100, # 每隔100步打印一次日志\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset, # 传入验证集\n",
    ")\n",
    "\n",
    "print(\"--- 开始训练 Gemma 模型 ---\")\n",
    "trainer.train()\n",
    "print(\"--- 训练完成 ---\")\n",
    "\n",
    "trainer.save_model(\"./my_gemma_lora_adapter\")\n",
    "print(\"LoRA 适配器已保存到 ./my_gemma_lora_adapter 目录。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5032b697-9e60-4630-b1c9-5a4dbcfc8a6a",
   "metadata": {},
   "source": [
    "# **6. 训练模型打包** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2434f687-4c01-49ee-a816-a914328c6ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tywin/my_jupyter_project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.04it/s]\n",
      "Unloading and merging model: 100%|█████████████████████████████████████████████████| 367/367 [00:00<00:00, -2162.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "微调后的模型已合并并保存到 ../merged_gemma_model 目录。\n",
      "现在你可以像使用普通模型一样加载这个目录进行推理。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# 1. 加载基础模型和分词器\n",
    "base_model_path = \"/home/tywin/my_jupyter_project/gemma-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "\n",
    "# 2. 加载 LoRA 适配器\n",
    "lora_adapter_path = \"./my_gemma_lora_adapter\"\n",
    "model = PeftModel.from_pretrained(base_model, lora_adapter_path)\n",
    "\n",
    "# 3. 将适配器合并到基础模型中\n",
    "# `merge_and_unload()` 方法会创建一个新的、完整的模型\n",
    "merged_model = model.merge_and_unload(progressbar=True)\n",
    "\n",
    "# 4. 保存合并后的模型和分词器\n",
    "merged_model_dir = \"../merged_gemma_model\" #\n",
    "merged_model.save_pretrained(merged_model_dir)\n",
    "tokenizer.save_pretrained(merged_model_dir)\n",
    "\n",
    "print(f\"微调后的模型已合并并保存到 {merged_model_dir} 目录。\")\n",
    "print(\"现在你可以像使用普通模型一样加载这个目录进行推理。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bed0e5-3ead-4759-9092-b93cba652948",
   "metadata": {},
   "source": [
    "# **7. 训练模型打包** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac941b21-8ee8-449e-b454-e452f44ef39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: merged_gemma_model\n",
      "INFO:hf-to-gguf:Model architecture: GemmaForCausalLM\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.float32 --> F16, shape = {2048, 256000}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.float32 --> F16, shape = {2048, 16384}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.float32 --> F16, shape = {2048, 256}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00003.safetensors'\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.float32 --> F16, shape = {16384, 2048}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "WARNING:gguf.vocab:Unknown separator token '<bos>' in TemplateProcessing<pair>\n",
      "INFO:gguf.vocab:Setting special token type bos to 2\n",
      "INFO:gguf.vocab:Setting special token type eos to 1\n",
      "INFO:gguf.vocab:Setting special token type unk to 3\n",
      "INFO:gguf.vocab:Setting special token type pad to 0\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "INFO:gguf.vocab:Setting add_sep_token to False\n",
      "INFO:gguf.vocab:Setting add_eos_token to False\n",
      "WARNING:gguf.vocab:Unknown separator token '<bos>' in TemplateProcessing<pair>\n",
      "WARNING:gguf.vocab:No handler for special token type prefix with id 67 - skipping\n",
      "WARNING:gguf.vocab:No handler for special token type suffix with id 69 - skipping\n",
      "WARNING:gguf.vocab:No handler for special token type middle with id 68 - skipping\n",
      "WARNING:gguf.vocab:No handler for special token type fsep with id 70 - skipping\n",
      "INFO:gguf.vocab:Setting special token type eot to 107\n",
      "INFO:gguf.vocab:Setting add_bos_token to True\n",
      "WARNING:gguf.gguf_writer:Duplicated key name 'tokenizer.ggml.add_bos_token', overwriting it with new value True of type BOOL\n",
      "INFO:gguf.vocab:Setting add_sep_token to False\n",
      "WARNING:gguf.gguf_writer:Duplicated key name 'tokenizer.ggml.add_sep_token', overwriting it with new value False of type BOOL\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:gemma-2b-finetuned.gguf: n_tensors = 164, total_size = 5.0G\n",
      "Writing: 100%|██████████| 5.01G/5.01G [00:08<00:00, 575Mbyte/s] \n",
      "INFO:hf-to-gguf:Model successfully exported to gemma-2b-finetuned.gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion successful!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Get the absolute path to your home directory\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "\n",
    "# Define the absolute paths to the script and input model\n",
    "script_path = os.path.join(home_dir, \"my_jupyter_project\", \"venv\", \"llama.cpp\", \"convert_hf_to_gguf.py\")\n",
    "input_path = os.path.join(home_dir, \"my_jupyter_project\", \"merged_gemma_model\")\n",
    "\n",
    "# Define the command and arguments\n",
    "cmd = [\n",
    "    \"python\",\n",
    "    script_path,\n",
    "    input_path,\n",
    "    \"--outfile\",\n",
    "    \"gemma-2b-finetuned.gguf\",\n",
    "    \"--outtype\",\n",
    "    \"f16\",\n",
    "]\n",
    "\n",
    "# Run the command from your home directory to avoid any relative path issues\n",
    "try:\n",
    "    print(\"Starting model conversion...\")\n",
    "    subprocess.run(cmd, check=True, cwd=os.path.join(home_dir, \"my_jupyter_project\"))\n",
    "    print(\"Conversion successful!\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"An error occurred. Make sure your paths are correct. {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b69aa4-46d6-4a14-ba66-c6cd915db68e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
